{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that splits data into train, validation, and test sets\n",
    "def train_val_test_split(X, Y, train_split=0.8, val_split=0.1, test_split=0.1, random_seed=seed):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_split, random_state=random_seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_split/(train_split+val_split), random_state=random_seed)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "# evaluation function\n",
    "def evaluation(y_pred, y_test):\n",
    "    valence_predicted = np.array(y_pred)[:, 0] \n",
    "    arousal_predicted = np.array(y_pred)[:, 1] \n",
    "    valence_test = np.array(y_test)[:, 0] \n",
    "    arousal_test = np.array(y_test)[:, 1] \n",
    "    # MSE\n",
    "    valence_mse = mean_squared_error(valence_test, valence_predicted)\n",
    "    arousal_mse = mean_squared_error(arousal_test, arousal_predicted)\n",
    "    # RMSE\n",
    "    valence_rmse = np.sqrt(valence_mse)\n",
    "    arousal_rmse = np.sqrt(arousal_mse)\n",
    "    # MAE\n",
    "    valence_mae = mean_absolute_error(valence_test, valence_predicted)\n",
    "    arousal_mae = mean_absolute_error(arousal_test, arousal_predicted)\n",
    "    # R^2 Score\n",
    "    valence_r2 = r2_score(valence_test, valence_predicted)\n",
    "    arousal_r2 = r2_score(arousal_test, arousal_predicted)\n",
    "\n",
    "    print(\"Valence MSE:\", valence_mse)\n",
    "    print(\"Arousal MSE:\", arousal_mse)\n",
    "    print(\"Valence RMSE:\", valence_rmse)\n",
    "    print(\"Arousal RMSE:\", arousal_rmse)\n",
    "    print(\"Valence MAE:\", valence_mae)\n",
    "    print(\"Arousal MAE:\", arousal_mae)\n",
    "    print(\"Valence R^2 Score:\", valence_r2)\n",
    "    print(\"Arousal R^2 Score:\", arousal_r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"../data/processed_multi_modal.csv\")\n",
    "\n",
    "audio_features_top4 = [\"loudness\", \"instrumentalness\", \"time_signature\", \"energy\"]\n",
    "audio_features_top9 = [\"loudness\", \"instrumentalness\", \"time_signature\", \"energy\", \"danceability\", \"tempo\", \"acousticness\", \"key\", \"speechiness\"]\n",
    "audio_features_all  = [\"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"tempo\", \"time_signature\"]\n",
    "\n",
    "lyric_features = [\"compound\",\"neg\", \"pos\", \"neu\", \"pca_tfidf0\", \"pca_tfidf1\", \"pca_tfidf2\", \"pca_tfidf3\", \"pca_tfidf4\", \"pca_tfidf5\", \"pca_tfidf6\", \"pca_tfidf7\", \n",
    "                  \"pca_tfidf8\", \"pca_tfidf9\", \"pca_tfidf10\", \"pca_tfidf11\", \"pca_tfidf12\", \"pca_tfidf13\", \"pca_tfidf14\", \"pca_tfidf15\", \"pca_tfidf16\", \"pca_tfidf17\", \n",
    "                  \"pca_tfidf18\", \"pca_tfidf19\", \"pca_tfidf20\", \"pca_tfidf21\", \"pca_tfidf22\", \"pca_tfidf23\", \"pca_tfidf24\", \"pca_tfidf25\", \"pca_tfidf26\", \"pca_tfidf27\", \n",
    "                  \"pca_tfidf28\", \"pca_tfidf29\", \"pca_tfidf30\", \"pca_tfidf31\", \"pca_tfidf32\", \"pca_tfidf33\", \"pca_tfidf34\", \"pca_tfidf35\", \"pca_tfidf36\", \"pca_tfidf37\", \n",
    "                  \"pca_tfidf38\", \"pca_tfidf39\", \"pca_tfidf40\", \"pca_tfidf41\", \"pca_tfidf42\", \"pca_tfidf43\", \"pca_tfidf44\", \"pca_tfidf45\", \"pca_tfidf46\", \"pca_tfidf47\", \n",
    "                  \"pca_tfidf48\", \"pca_tfidf49\"]\n",
    "\n",
    "ys_features = [\"valence\", \"arousal\"]\n",
    "\n",
    "df_audio = df_data[audio_features_top9]\n",
    "df_lyric = df_data[lyric_features]\n",
    "df_multi = df_data[audio_features_top9 + lyric_features]\n",
    "df_ys    = df_data[ys_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN-model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = df_multi, df_ys\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, Y)\n",
    "X_val_train, X_val_test, y_val_train, y_val_test = train_test_split(X_val, y_val, test_size=0.2, random_state=seed)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = torch.tensor(X_train.values, dtype=torch.float), torch.tensor(X_val.values, dtype=torch.float), torch.tensor(X_test.values, dtype=torch.float), torch.tensor(y_train.values, dtype=torch.float), torch.tensor(y_val.values, dtype=torch.float), torch.tensor(y_test.values, dtype=torch.float)\n",
    "X_val_train, X_val_test, y_val_train, y_val_test = torch.tensor(X_val_train.values, dtype=torch.float), torch.tensor(X_val_test.values, dtype=torch.float), torch.tensor(y_val_train.values, dtype=torch.float), torch.tensor(y_val_test.values, dtype=torch.float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, h0, h1, h2, output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, h0)\n",
    "        self.l2 = nn.Linear(h0, h1)\n",
    "        self.l3 = nn.Linear(h1, h2)\n",
    "        self.l4 = nn.Linear(h2, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l4(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "output_size = 2\n",
    "b_size = 32\n",
    "epochs = 500\n",
    "print_every = 100\n",
    "\n",
    "# wrapper function for training\n",
    "def train_model(h0, h1, h2, lr):\n",
    "    # model definition\n",
    "    nn_model = NeuralNet(input_size, h0, h1, h2, output_size)\n",
    "    \n",
    "    # loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=lr)\n",
    "    \n",
    "    # training loop\n",
    "    N = X_val_train.shape[0]\n",
    "    n_epochs = epochs\n",
    "    batch_size = b_size\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(0, N, batch_size):\n",
    "            inputs = X_val_train[i:i+batch_size]\n",
    "            labels = y_val_train[i:i+batch_size]\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = nn_model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print training progress\n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    y_val_pred = nn_model(X_val_test).detach()\n",
    "    score = mean_squared_error(y_val_pred, y_val_test)\n",
    "    # Return the trained model and validation score\n",
    "    return nn_model, score\n",
    "\n",
    "etas = [0.001, 0.01, 0.1]\n",
    "neuron_sizes = [[32,16,8],\n",
    "                [8,8,8],\n",
    "                [16,8,4],\n",
    "                [21,7,3]]\n",
    "model_lst = []\n",
    "for neurons in neuron_sizes:\n",
    "    for lr in etas:\n",
    "        model, score = train_model(neurons[0], neurons[1], neurons[2], lr)\n",
    "        model_lst.append((model, score, neurons, lr))\n",
    "        print(\"Neurons:\", neurons, \"Learning Rate:\", lr, \"Score:\", score)\n",
    "\n",
    "best_model, best_score, best_neurons, best_lr = min(model_lst, key=lambda x: x[1])\n",
    "best_model, best_score, best_neurons, best_lr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 500\n",
    "print_every = 100\n",
    "N = X_train.shape[0]\n",
    "\n",
    "\n",
    "best_nn_model = NeuralNet(input_size, best_neurons[0], best_neurons[1], best_neurons[2], output_size)\n",
    "# loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(best_nn_model.parameters(), lr=best_lr)\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, N, batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "        \n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = best_nn_model(inputs)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # print training process    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_nn_model(X_test).detach()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(y_pred.numpy(), y_test.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
